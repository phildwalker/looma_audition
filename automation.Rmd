---
title: "Scaling Analysis Considersations"
date: "`r paste0('Last Updated: ', format(Sys.time(), '%d %B, %Y')) `"
output:
  html_document:
    # toc: true
    # toc_float: true
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

# Situation
"Imagine that after this analysis is complete, you learn that we plan to run this type of experiment frequently in the future, perhaps dozens of times a month."

# Considersations
In prior roles when the discussion of some degree of scaling up/ "productionizing" an analysis I have found the highest degree of success when I understand customer goals. Though sometimes expressed as wanting "everything yesterday", I've found that the heart of the ask falls in which of the two potential paths: "Increasing the Speed" or "Expanding the Scope" (though often that ask falls in a degree of both).

Foundational to both of the potential paths above is understanding the fidelity of the data source(s)/ data pipeline. If there is low confidence in the data source, there is probably going to need to be a fair amount of work with the customer to either stabilize the data generation/sharing process or in the alignment of customer expectations with the data reality (the GIGO conversation).

Finally if there is a degree of stability in both the input and output expectations, to help improve the code sharability/ development cycle I would look to ensure we were using functions + internal packages where possible to help produce consistent outputs.
  
<br>    

## Increasing Speed of Outputs
In terms of the decreasing the time from data to insight, I've found success working along a spectrum of potential methods depending on the nature of the work. This spectrum ranges from:   

1. Deploying shiny app where the customer has access to a fairly standardized consistent output with updated data sources when that data becomes available.  
    * __Benefits:__ Fastest 
    * __Drawbacks:__ Most robust if desired output + incoming data source is consistent  
1. On-demand report via parameterized reporting   
    * __Benefits:__ Development cycle likely faster than a shiny app and would easily allow for an offline report to be outputted (easier distribution to external customers).  
    * __Drawbacks:__ This framework would likely depend on having RStudio Connect deployed (and $ invested)/ would likely not be used for external customers    
1. Setting up an "analyst in the loop" with scheduled jobs. Would want to consider infrastructure used.. using windows or linux (scheduler vs cron job), which would likely kick off a makefile/ setting up github actions. The reports would then output locally (send email when complete to alert),and would then be available for internal review/ revision/ further investigation before sharing with customer.  
    * __Benefits:__ Allows for adjusting to shifting demands/ inconsistent data sources before exposing variability to customer.
    * __Drawbacks:__ Slower feedback cycle to customer than other options above (though faster than doing manually).
  
  

## Expanding Scope
This path of work has the potential to greatly shift nature of the work (ie considerations around likely scope creep). In the conversation with the customer, I would look for a couple different themes in helping to determine the potential scope of work.  
 
__Potential Themes__   

1. More Store data:  
    * This could be a really easy change if the data source isn't changing, and we are essentially getting more data. In this case, I would validate that the data and analysis pipeline doesn't have inadvertent hard-coded selections (ie only looking for specific store ids, etc) and if not, than ideally would be able to refresh with minimal additional work.  
1. Different types of stores/ experiments/ additional variables:  
    * This could impact how the analysis is stratified, and I would communicate that there would likely be a little bit of pre-work before committing to the new body of work.








