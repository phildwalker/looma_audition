---
title: "Methodology"
date: "`r paste0('Last Updated: ', format(Sys.time(), '%d %B, %Y')) `"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: show
editor_options: 
  chunk_output_type: console
---

This is how the analysis was performed...  

* ingesting the dataset  
* exploring the results  
* developing the cleaned views  

Code will be visible here (probably show/hide code chunks)

```{r setup, include = F}
knitr::opts_chunk$set(echo = T, message = FALSE, warning = FALSE)
knitr::opts_chunk$set(fig.align = 'center')

library(tidyverse)
library(patchwork)
library(pdwtheme)
library(gt)
# To Turn off Scientific Notation Use this.
options(scipen = 999)
theme_set(theme_minimal())

```


# Read In Data {.tabset .tabset-pills}

```{r}
dat <- read_csv(here::here("data-raw", "looma_audition_data.csv"))

colnames <- colnames(dat)
```

## Data Notes

The csv file provided contains sample data for analysis. We receive raw data at the transaction level, with one row per customer and item purchased, with timestamps. The sample data is an aggregate of this at the “promotional period” level. Each campaign runs for one promotional period at a time. These periods vary between 3 and 5 weeks in length, so we provide sales in this data as “weekly velocity”, meaning average sales per week during each period.

Specifically, the dataset contains these columns:  

- store_group : which of the three experimental conditions the store falls in  
- store_id : ID unique to a physical store   
- promo_period_int : unique promo period number ordered in time (this doesn’t provide information you can’t get from period_num and period_year, but it’s convenient)    
- period_num : number of the promo period (there are 13 every year)  
- period_year: year the period falls in (along with period_num, unique identifies the promo period)  
- weekly_velocity_amount : average weekly dollar sales  
- weekly_velocity_quantity : average weekly unit sales  


The data provided is all for one featured product from Brewery A. The experiment was run during period number 11 of 2020. Data is provided back to mid-2018, but don’t feel compelled to incorporate it all in your analysis. What data is most relevant is up to you.

## Summary of Data

```{r}
glimpse(dat)
```

## Check for missingness

```{r}
naniar::vis_miss(dat)
```

# First Pass Review {.tabset .tabset-pills}


## Distributions of Data

```{r}

reviewCol <- 
  purrr::map(colnames,
             ~ dat %>% ggplot(aes_string(.x, fill="store_group"))+
               geom_bar()+
               scale_fill_pdw()+
               labs(title = glue::glue("Distribution for variable: {.x}"))+
               theme(plot.title = element_text(size = 15))
)

# reviewCol[[2]] #this is "iter 1"
reviewCol

```

## Correlations

```{r}

summary <- dat %>% 
  group_by(store_group) %>% 
  summarise(avg_amnt = mean(weekly_velocity_amount),
            n = n(),
            std_amnt = sd(weekly_velocity_amount))

dat %>% 
  ggplot(aes(store_group, weekly_velocity_amount, fill = store_group, color = store_group))+
    geom_jitter(size = 0.5, width = .3)+
    geom_violin(alpha=0.7)+
    geom_point(data = summary, aes(store_group, avg_amnt), color= "black", size = 2.5)


summary %>% 
  gt() %>% 
  fmt_number(columns = c(2,4), decimals = 2)

```



# Investigatings Effect


## Is there an overall improvement?


## Is there a potential store dependency?
Can we know if that's the case?















