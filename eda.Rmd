---
title: "Methodology"
date: "`r paste0('Last Updated: ', format(Sys.time(), '%d %B, %Y')) `"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: show
editor_options: 
  chunk_output_type: console
---

# Goal

This section is meant to help show my general process around:

* ingesting the dataset  
* exploring the results  
* developing the cleaned views  

I am intentionally not spending a ton of time working to clean up the visuals or extensively documenting the results, but I have enable the "show code" functionality as part of the yaml header for this rmd file (eda.Rmd). 

If desired the entirity of the website can be found on github [https://github.com/phildwalker/looma_audition]



```{r setup, include = F}
knitr::opts_chunk$set(echo = T, message = FALSE, warning = FALSE)
knitr::opts_chunk$set(fig.align = 'center')

library(tidyverse)
library(patchwork)
library(pdwtheme)
library(gt)
library(infer)
# To Turn off Scientific Notation Use this.
options(scipen = 999)

source("./src/looma_theme.R")

# theme_set(theme_minimal())



```


# Read In Data {.tabset .tabset-pills}

```{r}
dat <- read_csv(here::here("data-raw", "looma_audition_data.csv"))

colnames <- colnames(dat)
```

## Data Notes

The csv file provided contains sample data for analysis. We receive raw data at the transaction level, with one row per customer and item purchased, with timestamps. The sample data is an aggregate of this at the “promotional period” level. Each campaign runs for one promotional period at a time. These periods vary between 3 and 5 weeks in length, so we provide sales in this data as “weekly velocity”, meaning average sales per week during each period.

Specifically, the dataset contains these columns:  

- store_group : which of the three experimental conditions the store falls in  
- store_id : ID unique to a physical store   
- promo_period_int : unique promo period number ordered in time (this doesn’t provide information you can’t get from period_num and period_year, but it’s convenient)    
- period_num : number of the promo period (there are 13 every year)  
- period_year: year the period falls in (along with period_num, unique identifies the promo period)  
- weekly_velocity_amount : average weekly dollar sales  
- weekly_velocity_quantity : average weekly unit sales  


The data provided is all for one featured product from Brewery A. The experiment was run during period number 11 of 2020. Data is provided back to mid-2018, but don’t feel compelled to incorporate it all in your analysis. What data is most relevant is up to you.

## Summary of Data

```{r}
glimpse(dat)
```

## Check for missingness

Reviewing the chart below we see that we do not have any concerns about variables with missing data, therefore we will not have to take into consideration ways to mitigate/ adjust for missing data.

```{r}
naniar::vis_miss(dat)
```

# First Pass Review {.tabset .tabset-pills}


## Distributions of Data

Using purrr to run through the distribution of each variable by store group (as we know that is the main indicator for this analysis).

```{r}

reviewCol <- 
  purrr::map(colnames,
             ~ dat %>% ggplot(aes_string(.x, fill="store_group"))+
               geom_bar()+
               scale_fill_pdw()+
               labs(title = glue::glue("Distribution for variable: {.x}"))+
               looma_theme()
               # theme(plot.title = element_text(size = 15))
)

# reviewCol

reviewCol[[1]]
reviewCol[[2]] + geom_bar(width = 0.0001, alpha = 0.4, color= "black")+ facet_wrap(store_group ~. , ncol =  1)

# the store distribution seems similar for 2019 and 2020
reviewCol[[2]] + geom_bar(width = 0.0001, alpha = 0.4, color= "black")+ facet_grid(store_group ~factor(period_year))

reviewCol[[3]]
reviewCol[[4]]
reviewCol[[4]] + facet_wrap(factor(period_year) ~ ., ncol = 3)

reviewCol[[5]]

# no zero sales... which makes sense, but it means the data is naturally censored
reviewCol[[6]] + geom_bar(width = 4)

reviewCol[[6]] + geom_bar(width = 4)+ facet_wrap(factor(period_year) ~ ., ncol = 3)

reviewCol[[7]]


```

## Avg Velocity Amount

This summary would seem to indicate that the mean of the weekly velocity amount (average weekly dollars in sales) were higher for s

```{r}

summary <- dat %>% 
  group_by(store_group) %>% 
  summarise(avg_amnt = mean(weekly_velocity_amount),
            n = n(),
            std_amnt = sd(weekly_velocity_amount))

dat %>% 
  ggplot(aes(store_group, weekly_velocity_amount, fill = store_group, color = store_group))+
    geom_jitter(size = 0.5, width = .3)+
    geom_violin(alpha=0.7)+
    geom_point(data = summary, aes(store_group, avg_amnt), color= "black", size = 2.5)+
    labs(title = "Comparison of Weekly Velocity Amount by Store Group")+
    scale_fill_pdw()+
    scale_color_pdw()+
    looma_theme()


summary %>% 
  gt() %>% 
  fmt_number(columns = c(2,4), decimals = 2)

```

Using an anova to see if the differences are statistically significant. We would conclude that one or more of the group's weekly velocity amounts are statisically different

```{r}

observed_f_statistic <-dat %>%
  specify(weekly_velocity_amount ~ store_group) %>%
  calculate(stat = "F")


# visualize the theoretical null distribution and test statistic
dat %>%
  specify(weekly_velocity_amount ~ store_group) %>%
  hypothesize(null = "independence") %>%
  visualize(method = "theoretical") + 
  shade_p_value(observed_f_statistic,
                direction = "greater")

# generate the null distribution using randomization
null_distribution <- dat %>%
  specify(weekly_velocity_amount ~ store_group) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "F")

# calculate the p value from the observed statistic and null distribution
p_value <- null_distribution %>%
  get_p_value(obs_stat = observed_f_statistic,
              direction = "greater")

p_value

```

## Avg Velocity Quantity

This summary would seem to indicate that the mean of the weekly velocity amount (average weekly dollars in sales) were higher for s

```{r}

summary <- dat %>% 
  group_by(store_group) %>% 
  summarise(avg_amnt = mean(weekly_velocity_quantity),
            n = n(),
            std_amnt = sd(weekly_velocity_quantity))

dat %>% 
  ggplot(aes(store_group, weekly_velocity_quantity, fill = store_group, color = store_group))+
    geom_jitter(size = 0.5, width = .3)+
    geom_violin(alpha=0.7)+
    geom_point(data = summary, aes(store_group, avg_amnt), color= "black", size = 2.5)+
    labs(title = "Comparison of Weekly Velocity Quantity by Store Group")+
    scale_fill_pdw()+
    scale_color_pdw()+
    looma_theme()


summary %>% 
  gt() %>% 
  fmt_number(columns = c(2,4), decimals = 2)

```

Using an anova to see if the differences are statistically significant.

```{r}

observed_f_statistic <-dat %>%
  specify(weekly_velocity_quantity ~ store_group) %>%
  calculate(stat = "F")


# visualize the theoretical null distribution and test statistic
dat %>%
  specify(weekly_velocity_quantity ~ store_group) %>%
  hypothesize(null = "independence") %>%
  visualize(method = "theoretical") + 
  shade_p_value(observed_f_statistic,
                direction = "greater")

# generate the null distribution using randomization
null_distribution <- dat %>%
  specify(weekly_velocity_quantity ~ store_group) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "F")

# calculate the p value from the observed statistic and null distribution
p_value <- null_distribution %>%
  get_p_value(obs_stat = observed_f_statistic,
              direction = "greater")

p_value

```


# Investigating Effect {.tabset .tabset-pills}

## By Group and by Year 

Visually it looks like across conditions, that the sales are increasing YOY.



```{r}
summary <- dat %>% 
  mutate(period_year = as.factor(period_year)) %>% 
  group_by(store_group, period_year) %>% 
  summarise(avg_amnt = mean(weekly_velocity_quantity),
            n = n(),
            std_amnt = sd(weekly_velocity_quantity))

dat %>% 
  mutate(period_year = as.factor(period_year)) %>% 
  ggplot(aes(period_year, weekly_velocity_quantity, fill = period_year, color = period_year))+
    geom_jitter(size = 0.5, width = .3)+
    geom_violin(alpha=0.7)+
    geom_point(data = summary, aes(period_year, avg_amnt), color= "black", size = 2.5)+
    labs(title = "Comparison of Weekly Velocity Quantity by Store Group")+
    scale_fill_pdw()+
    scale_color_pdw()+
    looma_theme()+
    facet_wrap(store_group ~ ., ncol =3)

```

It's interesting that the film control group doesn't seem to see the same YOY increase for 2019 -> 2020 as the other two conditions.

```{r}
dat %>% 
  mutate(period_year = as.factor(period_year)) %>% 
  group_by(store_group, period_year) %>% 
  summarise(avg_amnt = mean(weekly_velocity_quantity),
            avg_dol = mean(weekly_velocity_amount)) %>%
  ungroup() %>% 
  pivot_wider(names_from = "period_year", values_from = c("avg_amnt", "avg_dol")) %>% 
  gt() %>% 
  fmt_number(columns = 2:7, decimals = 2)
```


## Linear model for yr + group (Amount)

```{r}
lm_dat <- dat %>% 
  mutate(period_year = as.character(period_year))

lm.model <- lm(data = lm_dat, 
               formula = weekly_velocity_quantity ~ 0 + period_year*store_group)

summary(lm.model)

summary(aov(lm.model))

```

## Tukey HSD (Amount)

* There doesn't seem to be a significant difference for any group in 2018
* Program_test is an improvement of 1 for 2019 as compared to the baseline
* Program_test is an improvement of 1.2 for 2020


```{r}
HSD <- TukeyHSD(aov(lm.model))$`period_year:store_group`

as.data.frame(HSD) %>% 
  rownames_to_column("Comparison") %>% 
  mutate(GroupComp = Comparison) %>% 
  separate(GroupComp, sep = "-", into = c("first", "second")) %>% 
  separate(first, sep = ":", into = c("year_1", "cond_1")) %>% 
  separate(second, sep = ":", into = c("year_2", "cond_2")) %>% 
  filter(year_1 == year_2) %>% 
  arrange(year_1) %>% 
  select(-year_1, -cond_1, -year_2, -cond_2) %>% 
  gt() %>% 
  fmt_number(columns = 2:4, decimals = 3) %>% 
  fmt_number(columns = 5, decimals = 5) %>% 
    tab_style(
    style = cell_fill(color = "lightgreen"),
    locations = cells_body(
      rows = `p adj` < 0.05)
  ) 

```


## Linear model for yr + group (Dollar)

```{r}
lm_dat <- dat %>% 
  mutate(period_year = as.character(period_year))

lm.model <- lm(data = lm_dat, 
               formula = weekly_velocity_amount ~ 0 + period_year*store_group)

summary(lm.model)

summary(aov(lm.model))

```

## Tukey HSD (Dollar)

* There doesn't seem to be a significant difference for any group in 2018
* Program_test is an improvement of 1 for 2019 as compared to the baseline
* Program_test is an improvement of 1.2 for 2020


```{r}
HSD <- TukeyHSD(aov(lm.model))$`period_year:store_group`

as.data.frame(HSD) %>% 
  rownames_to_column("Comparison") %>% 
  mutate(GroupComp = Comparison) %>% 
  separate(GroupComp, sep = "-", into = c("first", "second")) %>% 
  separate(first, sep = ":", into = c("year_1", "cond_1")) %>% 
  separate(second, sep = ":", into = c("year_2", "cond_2")) %>% 
  filter(year_1 == year_2) %>% 
  arrange(year_1) %>% 
  select(-year_1, -cond_1, -year_2, -cond_2) %>% 
  gt() %>% 
  fmt_number(columns = 2:4, decimals = 3) %>% 
  fmt_number(columns = 5, decimals = 5) %>% 
    tab_style(
    style = cell_fill(color = "lightgreen"),
    locations = cells_body(
      rows = `p adj` < 0.05)
  ) 

```





### Are stores represented evenly across the time period?

```{r}

dat %>% 
  count(store_id, store_group, period_year) %>% 
  pivot_wider(names_from= period_year, values_from = n) %>% 
  DT::datatable(rownames = F, filter = c("top"))


```


After the 9th period of 2018 it looks like the percent of non-baseline stores is fairly consistent.

```{r}
dat %>% 
  mutate(date_us_week = as.Date(paste(period_year, period_num * 4, 1), format = "%Y %U %u")) %>% 
  count(store_group, date_us_week, period_year, period_num) %>% 
  ggplot(aes(period_num, n, fill = store_group))+
    geom_bar(stat = "identity", position = "fill")+
    scale_y_continuous(labels = scales::percent)+
    # scale_x_date(date_breaks = "1 months")+
    scale_fill_pdw()+
    scale_color_pdw()+
    looma_theme()+
    labs(title = "Percent of Stores within each condition by period")+
    facet_wrap(period_year ~ ., ncol =3)+
    # theme(axis.text.x = element_text(angle = 60))+
    NULL

```





## Average Sales by Period

__Average Sales Dollars by Period__

2018... maybe just had some interest from the "innovation"  
2019 is interesting, and would follow my bias the most with the film_control and program_test following similar trends at different magnitudes.
2020 is odd, where it seems to be having an entirely different pattern than expected, with average sale dollars spiking up for the second half of the year in the "program_test" condition. 


```{r, out.width="100%", fig.width=12}
dat %>% 
  mutate(date_us_week = as.Date(paste(period_year, period_num * 4, 1), format = "%Y %U %u")) %>% 
  # mutate(period_year = as.factor(period_year)) %>% 
  group_by(date_us_week,period_year, store_group ) %>% 
  summarize(avg_amnt = mean(weekly_velocity_quantity),
            avg_dol = mean(weekly_velocity_amount),
            sd_dol = sd(weekly_velocity_amount)) %>% 
  ggplot(aes(date_us_week, avg_dol, color = store_group))+
    geom_line(size = 1.2)+
    geom_point(size = 1.5)+
    geom_errorbar(aes(ymin = avg_dol - sd_dol, ymax = avg_dol + sd_dol), width = 9, position = "dodge")+
    labs(title = "Avg Weekly Dollars For Store Group Over Time")+
    scale_fill_pdw()+
    scale_color_pdw()+
    scale_x_date(date_breaks = "1 months")+
    looma_theme()+
    facet_wrap(period_year ~ ., ncol =3, scales = "free_x")+
    theme(axis.text.x = element_text(angle = 60))+
    NULL


```


__Average Sales Amount by Period__


```{r, out.width="100%", fig.width=12}
dat %>% 
  mutate(date_us_week = as.Date(paste(period_year, period_num * 4, 1), format = "%Y %U %u")) %>% 
  # mutate(period_year = as.factor(period_year)) %>% 
  group_by(date_us_week,period_year, store_group ) %>% 
  summarize(avg_amnt = mean(weekly_velocity_quantity),
            sd_amnt = sd(weekly_velocity_quantity),
            avg_dol = mean(weekly_velocity_amount),
            sd_dol = sd(weekly_velocity_amount)) %>% 
  ggplot(aes(date_us_week, avg_amnt, color = store_group))+
    geom_line(size = 1.2)+
    geom_point(size = 1.5)+
    geom_errorbar(aes(ymin = avg_amnt - sd_amnt, ymax = avg_amnt + sd_amnt), width = 9, position = "dodge")+
    labs(title = "Avg Weekly Sales Amount For Store Group Over Time")+
    scale_fill_pdw()+
    scale_color_pdw()+
    scale_x_date(date_breaks = "1 months")+
    looma_theme()+
    facet_wrap(period_year ~ ., ncol =3, scales = "free_x")+
    theme(axis.text.x = element_text(angle = 60))+
    NULL


```


## Experiment Review
Comment in the prompt: "The experiment was run during period number 11 of 2020"

... I think this means that that's when the data was pulled for investigation (because there isn't any data after this time period)... Though it's possible that period 11 for 2020 wasn't complete...

```{r}
sumWeek <- dat %>% 
  mutate(date_us_week = as.Date(paste(period_year, period_num * 4, 1), format = "%Y %U %u")) %>% 
  # mutate(period_year = as.factor(period_year)) %>% 
  mutate(ExperimentPer = ifelse(period_year == 2020 & period_num == 11, 1, 0)) %>% 
  group_by(date_us_week,period_year,period_num, store_group, ExperimentPer ) %>% 
  summarize(avg_amnt = mean(weekly_velocity_quantity),
            sd_amnt = sd(weekly_velocity_quantity),
            avg_dol = mean(weekly_velocity_amount),
            sd_dol = sd(weekly_velocity_amount))

sumWeek %>% filter(ExperimentPer == 1)
  

```

## Is there a potential store dependency?

Not sure if we can know if this is the case or not because we only see stores in one condition or the other.














