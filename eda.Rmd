---
title: "Methodology"
date: "`r paste0('Last Updated: ', format(Sys.time(), '%d %B, %Y')) `"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: show
editor_options: 
  chunk_output_type: console
---

This is how the analysis was performed...  

* ingesting the dataset  
* exploring the results  
* developing the cleaned views  

Code will be visible here (probably show/hide code chunks)

```{r setup, include = F}
knitr::opts_chunk$set(echo = T, message = FALSE, warning = FALSE)
knitr::opts_chunk$set(fig.align = 'center')

library(tidyverse)
library(patchwork)
library(pdwtheme)
library(gt)
library(infer)
# To Turn off Scientific Notation Use this.
options(scipen = 999)

source("./src/looma_theme.R")

# theme_set(theme_minimal())



```


# Read In Data {.tabset .tabset-pills}

```{r}
dat <- read_csv(here::here("data-raw", "looma_audition_data.csv"))

colnames <- colnames(dat)
```

## Data Notes

The csv file provided contains sample data for analysis. We receive raw data at the transaction level, with one row per customer and item purchased, with timestamps. The sample data is an aggregate of this at the “promotional period” level. Each campaign runs for one promotional period at a time. These periods vary between 3 and 5 weeks in length, so we provide sales in this data as “weekly velocity”, meaning average sales per week during each period.

Specifically, the dataset contains these columns:  

- store_group : which of the three experimental conditions the store falls in  
- store_id : ID unique to a physical store   
- promo_period_int : unique promo period number ordered in time (this doesn’t provide information you can’t get from period_num and period_year, but it’s convenient)    
- period_num : number of the promo period (there are 13 every year)  
- period_year: year the period falls in (along with period_num, unique identifies the promo period)  
- weekly_velocity_amount : average weekly dollar sales  
- weekly_velocity_quantity : average weekly unit sales  


The data provided is all for one featured product from Brewery A. The experiment was run during period number 11 of 2020. Data is provided back to mid-2018, but don’t feel compelled to incorporate it all in your analysis. What data is most relevant is up to you.

## Summary of Data

```{r}
glimpse(dat)
```

## Check for missingness

Reviewing the chart below we see that we do not have any concerns about variables with missing data, therefore we will not have to take into consideration ways to mitigate/ adjust for missing data.

```{r}
naniar::vis_miss(dat)
```

# First Pass Review {.tabset .tabset-pills}


## Distributions of Data

```{r}

reviewCol <- 
  purrr::map(colnames,
             ~ dat %>% ggplot(aes_string(.x, fill="store_group"))+
               geom_bar()+
               scale_fill_pdw()+
               labs(title = glue::glue("Distribution for variable: {.x}"))+
               looma_theme()
               # theme(plot.title = element_text(size = 15))
)

# reviewCol[[2]] #this is "iter 1"
reviewCol

```

## Avg Velocity Amount

This summary would seem to indicate that the mean of the weekly velocity amount (average weekly dollars in sales) were higher for s

```{r}

summary <- dat %>% 
  group_by(store_group) %>% 
  summarise(avg_amnt = mean(weekly_velocity_amount),
            n = n(),
            std_amnt = sd(weekly_velocity_amount))

dat %>% 
  ggplot(aes(store_group, weekly_velocity_amount, fill = store_group, color = store_group))+
    geom_jitter(size = 0.5, width = .3)+
    geom_violin(alpha=0.7)+
    geom_point(data = summary, aes(store_group, avg_amnt), color= "black", size = 2.5)+
    labs(title = "Comparison of Weekly Velocity Amount by Store Group")+
    scale_fill_pdw()+
    scale_color_pdw()+
    looma_theme()


summary %>% 
  gt() %>% 
  fmt_number(columns = c(2,4), decimals = 2)

```

Using an anova to see if the differences are statistically significant. We would conclude that one or more of the group's weekly velocity amounts are statisically different

```{r}

observed_f_statistic <-dat %>%
  specify(weekly_velocity_amount ~ store_group) %>%
  calculate(stat = "F")


# visualize the theoretical null distribution and test statistic
dat %>%
  specify(weekly_velocity_amount ~ store_group) %>%
  hypothesize(null = "independence") %>%
  visualize(method = "theoretical") + 
  shade_p_value(observed_f_statistic,
                direction = "greater")

# generate the null distribution using randomization
null_distribution <- dat %>%
  specify(weekly_velocity_amount ~ store_group) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "F")

# calculate the p value from the observed statistic and null distribution
p_value <- null_distribution %>%
  get_p_value(obs_stat = observed_f_statistic,
              direction = "greater")

p_value

```

## Avg Velocity Quantity

This summary would seem to indicate that the mean of the weekly velocity amount (average weekly dollars in sales) were higher for s

```{r}

summary <- dat %>% 
  group_by(store_group) %>% 
  summarise(avg_amnt = mean(weekly_velocity_quantity),
            n = n(),
            std_amnt = sd(weekly_velocity_quantity))

dat %>% 
  ggplot(aes(store_group, weekly_velocity_quantity, fill = store_group, color = store_group))+
    geom_jitter(size = 0.5, width = .3)+
    geom_violin(alpha=0.7)+
    geom_point(data = summary, aes(store_group, avg_amnt), color= "black", size = 2.5)+
    labs(title = "Comparison of Weekly Velocity Quantity by Store Group")+
    scale_fill_pdw()+
    scale_color_pdw()+
    looma_theme()


summary %>% 
  gt() %>% 
  fmt_number(columns = c(2,4), decimals = 2)

```

Using an anova to see if the differences are statistically significant.

```{r}

observed_f_statistic <-dat %>%
  specify(weekly_velocity_quantity ~ store_group) %>%
  calculate(stat = "F")


# visualize the theoretical null distribution and test statistic
dat %>%
  specify(weekly_velocity_quantity ~ store_group) %>%
  hypothesize(null = "independence") %>%
  visualize(method = "theoretical") + 
  shade_p_value(observed_f_statistic,
                direction = "greater")

# generate the null distribution using randomization
null_distribution <- dat %>%
  specify(weekly_velocity_quantity ~ store_group) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "F")

# calculate the p value from the observed statistic and null distribution
p_value <- null_distribution %>%
  get_p_value(obs_stat = observed_f_statistic,
              direction = "greater")

p_value

```



# Trends over time {.tabset .tabset-pills}

```{r}
dat <-
  dat %>% 
  mutate(period_yr_num = as.numeric(paste0(as.character(period_year), ".", as.character(period_num))))
```


# Are stores represented evenly across the time period?

```{r}

dat %>% 
  count(store_id, store_group, period_year) %>% 
  pivot_wider(names_from= period_year, values_from = n) %>% 
  DT::datatable(rownames = F, filter = c("top"))


```



# Investigatings Effect {.tabset .tabset-pills}


## Is there an overall improvement?


## Is there a potential store dependency?
Can we know if that's the case?















